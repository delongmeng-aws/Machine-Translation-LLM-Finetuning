{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fe1856-c907-405f-b6d4-afadb3e0b721",
   "metadata": {},
   "source": [
    "# Machine translation with fine-tuned t5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdecd67-d950-4e88-ad8a-5e67503813d7",
   "metadata": {},
   "source": [
    "This notebook provides an example solution for a Machine Translation task. This solution uses a large language model, the [google/flan-t5-xl model](https://huggingface.co/google/flan-t5-xl) (3B parameters) from the Hugging Face platform, to translate text from English to multiple target languages. \n",
    "\n",
    "Compute resource: AWS EC2 p5.48xlarge instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95674a",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb0896f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install -q ipykernel==6.22.0\n",
    "# # !pip3 install -q torch==2.0.1\n",
    "# !pip3 install -q transformers==4.28.1\n",
    "# !pip3 install -q bitsandbytes==0.39.0\n",
    "# !pip3 install -q peft==0.3.0\n",
    "# !pip3 install -q pytest==7.3.2\n",
    "# !pip3 install -q datasets==2.10.0\n",
    "# !pip3 install -q sentencepiece\n",
    "# !pip3 install -q accelerate\n",
    "# !pip3 install -q nltk\n",
    "\n",
    "# # install deepspeed and ninja for jit compilations of kernels\n",
    "# !pip3 install -q deepspeed ninja --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2579fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import tqdm\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "BLEU = 'bleu'\n",
    "\n",
    "language_mapping = {\"es\":\"Spanish\", \"de\":\"German\", \"fr\": \"French\", \"it\":\"Italian\", \"pt\":\"Portuguese\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d773a1-15ca-45be-8e22-fc9c0f417b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f8028191ce4e8586be7d5a57b30da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63ffd3a-1943-4632-93b2-944fb22f7b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6521adb8-ee95-4834-84f1-078da690eb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ac4a5c-06c1-4bed-8974-9ec01603a230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9be2045-2ec8-47c4-b6fc-34ef21052bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA H100 80GB HBM3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829104e4-cf49-4547-b6c5-3f3fff6c5529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303383fa",
   "metadata": {},
   "source": [
    "## Load Pretrained Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878efc10-0fef-4c81-8d93-180c422dd4a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2255a1204f16463194be6e1b623ebf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-xl\" # Hugging Face Model Id\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f52749",
   "metadata": {},
   "source": [
    "## Load dataset and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17c26a",
   "metadata": {},
   "source": [
    "The dataset has the following columns: \n",
    "- `ID`\n",
    "- `input_to_translate`: the source sentence in English\n",
    "- `label`: the translation reference in the target language\n",
    "- `gender`: f(emale) or m(ale)\n",
    "- `language_pair`: `<source>_<target>`, such as en_fr for English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e182536b-e94a-4d6d-9105-bd51377cbda1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>input_to_translate</th>\n",
       "      <th>label</th>\n",
       "      <th>gender</th>\n",
       "      <th>language_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>She started training for the biathlon in 2003.</td>\n",
       "      <td>Comenzó a entrenar para el biatlón en 2003.</td>\n",
       "      <td>f</td>\n",
       "      <td>en_es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>He joined Philippine Airlines as a trainee pil...</td>\n",
       "      <td>Er wurde Flugschüler bei Philippine Airlines u...</td>\n",
       "      <td>m</td>\n",
       "      <td>en_de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                 input_to_translate  \\\n",
       "0   0     She started training for the biathlon in 2003.   \n",
       "1   1  He joined Philippine Airlines as a trainee pil...   \n",
       "\n",
       "                                               label gender language_pair  \n",
       "0        Comenzó a entrenar para el biatlón en 2003.      f         en_es  \n",
       "1  Er wurde Flugschüler bei Philippine Airlines u...      m         en_de  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features = pd.read_csv(\"data/training.csv\", encoding=\"utf-8-sig\")\n",
    "training_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4308b5f1-29bb-42e2-9285-f980c5d19697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(x):\n",
    "    language_mapping = {\"es\":\"Spanish\", \"de\":\"German\", \"fr\": \"French\", \"it\":\"Italian\", \"pt\":\"Portuguese\"}\n",
    "    source_text = x[\"input_to_translate\"]\n",
    "    language = x[\"language_pair\"].split('_')[1]\n",
    "    input_text = f\"Translate the following sentence from English to {language_mapping[language]}: \\\"{source_text}\\\" \"\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bee3d61-0134-41cd-bb9b-ed7d92044398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>input_to_translate</th>\n",
       "      <th>label</th>\n",
       "      <th>gender</th>\n",
       "      <th>language_pair</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>She started training for the biathlon in 2003.</td>\n",
       "      <td>Comenzó a entrenar para el biatlón en 2003.</td>\n",
       "      <td>f</td>\n",
       "      <td>en_es</td>\n",
       "      <td>Translate the following sentence from English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>He joined Philippine Airlines as a trainee pil...</td>\n",
       "      <td>Er wurde Flugschüler bei Philippine Airlines u...</td>\n",
       "      <td>m</td>\n",
       "      <td>en_de</td>\n",
       "      <td>Translate the following sentence from English ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                 input_to_translate  \\\n",
       "0   0     She started training for the biathlon in 2003.   \n",
       "1   1  He joined Philippine Airlines as a trainee pil...   \n",
       "\n",
       "                                               label gender language_pair  \\\n",
       "0        Comenzó a entrenar para el biatlón en 2003.      f         en_es   \n",
       "1  Er wurde Flugschüler bei Philippine Airlines u...      m         en_de   \n",
       "\n",
       "                                              prompt  \n",
       "0  Translate the following sentence from English ...  \n",
       "1  Translate the following sentence from English ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features[\"prompt\"] = training_features.apply(generate_prompt, axis=1)\n",
    "training_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1815555",
   "metadata": {},
   "source": [
    "#### Check the generated prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a447ce05-19c4-4dd8-ac49-3e7ae150c6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate the following sentence from English to Spanish: \"She started training for the biathlon in 2003.\" '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features.iloc[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193e11ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate the following sentence from English to German: \"He joined Philippine Airlines as a trainee pilot, and was later pirated by Boeing.\" '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features.iloc[1][\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf680f0",
   "metadata": {},
   "source": [
    "#### Load and generate prompt for test set\n",
    "\n",
    "The test set is smilar with the training set, except that it is lacking the \"label\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b80913-70c2-48b9-b97e-f2b165255e88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'input_to_translate', 'gender', 'language_pair']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = pd.read_csv(\"data/test_features.csv\", encoding=\"utf-8-sig\")\n",
    "list(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87e1759-1450-4c71-a22a-035dcb594ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'input_to_translate', 'gender', 'language_pair', 'prompt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features[\"prompt\"] = test_features.apply(generate_prompt, axis=1)\n",
    "list(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d01829-7df4-4a03-abf1-4d13829a3e2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use Hugging Face Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65540c5-d3b1-482b-bbb3-163917b15179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'input_to_translate', 'label', 'gender', 'language_pair', 'prompt'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_raw = datasets.Dataset.from_pandas(training_features, split=\"train\")\n",
    "train_ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7880e666-848c-4824-b2c1-80c6084a2f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'input_to_translate', 'gender', 'language_pair', 'prompt'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_raw = datasets.Dataset.from_pandas(test_features, split=\"test\")\n",
    "test_ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458ba112-0204-4149-bb31-d20ced1b0372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfa71f14-f2c5-434a-b622-6f7610de34d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18425e5",
   "metadata": {},
   "source": [
    "### Figure out token length and tokenize the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62b24e6f-b8bf-4770-8c01-ad266527c542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e03a6d9a4342f892c9f0136d57dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 155\n",
      "95% source length: 70\n"
     ]
    }
   ],
   "source": [
    "tokenized_source_training = train_ds_raw.map(\n",
    "    lambda x: tokenizer(x[\"prompt\"], truncation=True), \n",
    "    batched=True, remove_columns=['ID', 'input_to_translate', 'label', 'gender', 'language_pair', 'prompt'])\n",
    "\n",
    "source_lengths_training = [len(x) for x in tokenized_source_training[\"input_ids\"]]\n",
    "\n",
    "print(f\"Max source length: {max(source_lengths_training)}\")\n",
    "print(f\"95% source length: {int(np.percentile(source_lengths_training, 95))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56c56858-c316-47e5-bca9-fd5dc7ceaebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41862fe3fc34fd4b0d1784520ad3151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 273\n",
      "95% target length: 101\n"
     ]
    }
   ],
   "source": [
    "tokenized_target_training = train_ds_raw.map(\n",
    "    lambda x: tokenizer(x[\"label\"], truncation=True), \n",
    "    batched=True, remove_columns=['ID', 'input_to_translate', 'label', 'gender', 'language_pair', 'prompt'])\n",
    "target_lengths_training = [len(x) for x in tokenized_target_training[\"input_ids\"]]\n",
    "\n",
    "print(f\"Max target length: {max(target_lengths_training)}\")\n",
    "print(f\"95% target length: {int(np.percentile(target_lengths_training, 95))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "feb45054-4f59-4ead-b29b-ce9016942a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf101583d634202859cd535b09669a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length in test set: 155\n",
      "95% source length in test set: 76\n"
     ]
    }
   ],
   "source": [
    "tokenized_source_test = test_ds_raw.map(\n",
    "    lambda x: tokenizer(x[\"prompt\"], truncation=True), \n",
    "    batched=True, remove_columns=['ID', 'input_to_translate', 'gender', 'language_pair', 'prompt'])\n",
    "\n",
    "source_lengths_test = [len(x) for x in tokenized_source_test[\"input_ids\"]]\n",
    "\n",
    "print(f\"Max source length in test set: {max(source_lengths_test)}\")\n",
    "print(f\"95% source length in test set: {int(np.percentile(source_lengths_test, 95))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0e4968e-2f80-4351-8171-0956726294a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_source_length = max(max(source_lengths_training), max(source_lengths_test))\n",
    "max_source_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec520de4-0c21-4b7d-b6d1-336d53625e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_length = max(target_lengths_training)\n",
    "max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85131942-951d-4586-9a00-124906623502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference: https://www.philschmid.de/fine-tune-flan-t5-deepspeed\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(sample[\"prompt\"], max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"label\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a829e13-8b5a-4f48-8ec1-433f3719356b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66269d301c4b4cfbb46b34c5811b1c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_ds = train_ds_raw.map(\n",
    "    preprocess_function, batched=True, \n",
    "    remove_columns=['ID', 'input_to_translate', 'label', 'gender', 'language_pair', 'prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e83fda3a-78c1-4235-8751-7adb356464ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95f6db-73c1-4bbf-b1f4-889abc7eef87",
   "metadata": {},
   "source": [
    "### Split the original training set into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95cc840a-b944-4aad-b503-21b3d8dcf058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict = tokenized_train_ds.train_test_split(test_size=0.2)\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60204641-a607-4ae2-98a0-fa7c29e66a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 9600\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = ds_dict[\"train\"]\n",
    "trainset           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "255809ba-e636-44a4-8217-a125e4c7a853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2400\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = ds_dict[\"test\"]\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54d32b89-7a22-4e31-8ec8-851675b01318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea60625b2744450a365486cfc88947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd477a34523419ab42a45e9c27df520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save dataset to disk\n",
    "save_dataset_path = \"training_data\"\n",
    "trainset.save_to_disk(os.path.join(save_dataset_path,\"train\"))\n",
    "testset.save_to_disk(os.path.join(save_dataset_path,\"eval\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d73084-9bb6-4d58-97aa-085125387307",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "reference: https://www.philschmid.de/fine-tune-flan-t5-deepspeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e513319a-7486-47f3-b4bf-6582c68d6b1d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install -q pytesseract transformers datasets nltk tensorboard py7zr evaluate sacrebleu --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16e30975-e52d-4271-a5ef-9a6a4e1203e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccb74e4f-ab8b-4089-9228-983755b6f0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30d91446-8cad-4652-9c9c-a5be56e2f9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    # for some reason, also get a lot of -100 in preds\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff7427e1-0825-4215-b6fa-7209fd1c3d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a6009cb-1f66-46d8-8abe-e88d94cba8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ml/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ml did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ml/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 9.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ml/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eb3fb45-4862-4566-b04a-b45bd7929fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "repository_id = f\"{model_id.split('/')[1]}-finetuned-translation-10132023\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    generation_max_length=273,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0ecdd4b-6ced-481c-b973-21c3bf716ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=testset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e123d383-db35-41a0-9058-c7871d87cd81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 40:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.829607</td>\n",
       "      <td>33.380100</td>\n",
       "      <td>53.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.771901</td>\n",
       "      <td>35.555400</td>\n",
       "      <td>53.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.756788</td>\n",
       "      <td>36.106400</td>\n",
       "      <td>53.152500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.97969482421875, metrics={'train_runtime': 2427.1468, 'train_samples_per_second': 11.866, 'train_steps_per_second': 0.124, 'total_flos': 7.6970842914816e+16, 'train_loss': 0.97969482421875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd327418-f57c-4adb-be78-bf19581cc122",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "batch_size = 128 -> OutOfMemoryError: CUDA out of memory.\n",
    "\n",
    "batch_size = 96 -> OK\n",
    "\n",
    "(needed to add the replacement of -100 for preds as well in compute metrics; otherwise get IndexError of \"piece id is out of range\")\n",
    "\n",
    "\n",
    "each epoch ~13min (training 3min, evaluation ~10min)  \n",
    "3 epochs -> around 40min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8466d3af-bea3-4b23-9d6c-5ed7872c0119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 07:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7567881345748901,\n",
       " 'eval_bleu': 36.1064,\n",
       " 'eval_gen_len': 53.1525,\n",
       " 'eval_runtime': 560.4878,\n",
       " 'eval_samples_per_second': 4.282,\n",
       " 'eval_steps_per_second': 0.045,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88195c29-2b9d-4799-9dac-526d0a3f3eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flan-t5-xl-finetuned-translation-10132023'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f7c549b-8bdd-4e91-a255-bde1a902d72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b210e48afd745329bbe4f6577159af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b874375dbf4fdd9e715ce4245ab951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b381f0478a874753840c3c5199f8f1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/delmeng/flan-t5-xl-finetuned-translation-10132023/tree/main/'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847c931-84b5-4fe8-8fed-676fba641f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2c342a4-f293-42ad-b38c-bb478ec91ea4",
   "metadata": {},
   "source": [
    "## Use the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ab0d1c4-c1a4-4c37-a35a-52d5a064a5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc12c0381bd949e88392c427fa3ea876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-xl\" # Hugging Face Model Id\n",
    "repository_id = f\"delmeng/{model_id.split('/')[1]}-finetuned-translation-10132023\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(repository_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(repository_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f471b0d8-5aa0-4c58-b2c0-8b0e9efdd76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfc3a2",
   "metadata": {},
   "source": [
    "### Still use the \"translation\" pipeline task type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd6c5ab9-9190-42cd-852a-11885de2770a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccae1c3c080c460388b972220cd2b40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/envs/ml/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1013: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe_ft = pipeline(\"translation\", model = repository_id, max_length=tokenizer.model_max_length, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791c303",
   "metadata": {},
   "source": [
    "### Evaluation on a small subset of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9e35873-bc96-4a73-b14a-edaae41ba0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'input_to_translate', 'label', 'gender', 'language_pair', 'prompt'],\n",
       "    num_rows: 96\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 96\n",
    "train_ds_test = datasets.Dataset.from_pandas(training_features.head(sample_size), split=\"train\")\n",
    "train_ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc22138-4ca1-42ea-ab51-68c4876e758e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/96 [01:25<2:15:16, 85.44s/it]\n",
      "100%|██████████| 96/96 [01:54<00:00,  1.19s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "prediction = pd.DataFrame({\"ID\": pd.Series(dtype=\"int\"),\n",
    "                   \"predicted_label\": pd.Series(dtype=\"str\")})\n",
    "batch_size = 48\n",
    "# default batch size is 1, if not specified\n",
    "# with higher batch size, it's easier to trigger out of memory error\n",
    "\n",
    "for out in tqdm.tqdm(pipe_ft(KeyDataset(train_ds_test, \"prompt\"), batch_size=batch_size),total=len(train_ds_test)):\n",
    "# for out in pipe(KeyDataset(train_ds_raw, \"prompt\")):\n",
    "# for out in tqdm.tqdm(pipe(KeyDataset(train_ds_raw, \"prompt\"))):\n",
    "\n",
    "    #print(out)\n",
    "    generated_text = out[0]['translation_text']\n",
    "    predicted_labels.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4052a78b-74c7-4540-97f9-ecadcbfbf6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction[\"ID\"] = training_features.iloc[0:sample_size][\"ID\"]\n",
    "prediction[\"predicted_label\"] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f3541e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bleu_func(x, y):\n",
    "    chencherry = SmoothingFunction()\n",
    "    x_split = [x_entry.strip().split() for x_entry in x]\n",
    "    y_split = y.strip().split()\n",
    "    return sentence_bleu(x_split, y_split, smoothing_function=chencherry.method3)\n",
    "\n",
    "def bleu_custom(y_true, y_pred, groups):\n",
    "    joined = pd.concat([y_true, y_pred, groups], axis=1)\n",
    "    joined[BLEU] = joined.apply(lambda x: bleu_func([x[y_true.name]], x[y_pred.name]), axis=1)\n",
    "    values = [joined[joined[groups.name] == unique][BLEU].mean() for unique in unique_list]\n",
    "    print(f\"Overall mean: {joined[BLEU].mean()}\")\n",
    "    print(f\"Different genders: {values}\")\n",
    "    print(f\"Final score: {joined[BLEU].mean() - np.fabs(values[0] - values[1])/2}\")\n",
    "    return joined[BLEU].mean() - np.fabs(values[0] - values[1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69808986-ff20-4452-84f1-6ceb13b03e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.35987764995509036\n",
      "Different genders: [0.3603295420111593, 0.3592171923346818]\n",
      "Final score: 0.35932147511685164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35932147511685164"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_custom(\n",
    "    training_features.iloc[0:sample_size][\"label\"], \n",
    "    prediction[\"predicted_label\"], \n",
    "    training_features.iloc[0:sample_size][\"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bcc413-8b09-4500-bb50-1098727b7512",
   "metadata": {},
   "source": [
    "### Evaluation on test dataset using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46c67e5e-2440-420b-97ce-66a85d398211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [29:03<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "test_prediction = pd.DataFrame({\"ID\": pd.Series(dtype=\"int\"), \"label\": pd.Series(dtype=\"str\")})\n",
    "batch_size = 32\n",
    "# default batch size is 1, if not specified\n",
    "# with higher batch size, it's easier to trigger out of memory error\n",
    "\n",
    "for out in tqdm.tqdm(pipe_ft(KeyDataset(test_ds_raw, \"prompt\"), batch_size=batch_size),total=len(test_ds_raw)):\n",
    "    generated_text = out[0]['translation_text']\n",
    "    predicted_labels.append(generated_text)\n",
    "\n",
    "test_prediction[\"ID\"] = test_features[\"ID\"]\n",
    "test_prediction[\"label\"] = predicted_labels\n",
    "test_prediction.to_csv(\"t5_xl_finetuned_translation_submission-10142023.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c67bea-16bd-4d7f-b846-38d718fbb360",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "when batch size = 48, got OOM error at     51%|█████     | 1536/3000 [12:25<11:50,  2.06it/s]\n",
    "\n",
    "when batch size = 32 -> OK (30min)\n",
    "\n",
    "\n",
    "final score: 0.265392 (compare with 0.167 using the pretrained model without fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd62e3-20d1-4582-bf9c-d6839376a847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70f477fa-557f-48f0-9d2f-e0e8aea8946c",
   "metadata": {},
   "source": [
    "## Try the \"text2text-generation\" pipeline task type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d8ccceb-9de6-49ae-b44c-977a68358997",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec96c8b5b08844b3b4a8a0e0d8735a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipe_ft = pipeline(\"text2text-generation\", model = repository_id, max_length=tokenizer.model_max_length, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d85200",
   "metadata": {},
   "source": [
    "### Evaluation on a small subset of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f31193-9c80-4eb4-8072-bb51d1b74394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'input_to_translate', 'label', 'gender', 'language_pair', 'prompt'],\n",
       "    num_rows: 96\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 96\n",
    "train_ds_test = datasets.Dataset.from_pandas(training_features.head(sample_size), split=\"train\")\n",
    "train_ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9c116d-6239-449b-be56-503763b9e189",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/96 [00:26<41:21, 26.12s/it]\n",
      "100%|██████████| 96/96 [00:34<00:00,  2.80it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "prediction = pd.DataFrame({\"ID\": pd.Series(dtype=\"int\"),\n",
    "                   \"predicted_label\": pd.Series(dtype=\"str\")})\n",
    "batch_size = 48\n",
    "# default batch size is 1, if not specified\n",
    "# with higher batch size, it's easier to trigger out of memory error\n",
    "\n",
    "for out in tqdm.tqdm(pipe_ft(KeyDataset(train_ds_test, \"prompt\"), batch_size=batch_size),total=len(train_ds_test)):\n",
    "    generated_text = out[0]['generated_text']\n",
    "    predicted_labels.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c58ef1e-958b-4681-8739-8e0c724e28d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction[\"ID\"] = training_features.iloc[0:sample_size][\"ID\"]\n",
    "prediction[\"predicted_label\"] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a72353e2-422d-4549-b3b3-1e405e3b5b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.3452952905976548\n",
      "Different genders: [0.3550664310052024, 0.33101439307893155]\n",
      "Final score: 0.33326927163451936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33326927163451936"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_custom(\n",
    "    training_features.iloc[0:sample_size][\"label\"], \n",
    "    prediction[\"predicted_label\"], \n",
    "    training_features.iloc[0:sample_size][\"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b631db-28f9-4d0c-b43c-5f3fbf846593",
   "metadata": {},
   "source": [
    "### Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf13e5a6-e11c-43cb-b42e-4459daa1f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [14:51<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "test_prediction = pd.DataFrame({\"ID\": pd.Series(dtype=\"int\"), \"label\": pd.Series(dtype=\"str\")})\n",
    "batch_size = 32\n",
    "# default batch size is 1, if not specified\n",
    "# with higher batch size, it's easier to trigger out of memory error\n",
    "\n",
    "for out in tqdm.tqdm(pipe_ft(KeyDataset(test_ds_raw, \"prompt\"), batch_size=batch_size),total=len(test_ds_raw)):\n",
    "    generated_text = out[0]['generated_text']\n",
    "    predicted_labels.append(generated_text)\n",
    "\n",
    "test_prediction[\"ID\"] = test_features[\"ID\"]\n",
    "test_prediction[\"label\"] = predicted_labels\n",
    "test_prediction.to_csv(\"t5_xl_finetuned_text_submission-10142023.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8859cf-1282-4c28-8633-4deacf62b9ca",
   "metadata": {},
   "source": [
    "when batch size = 32 -> 15min\n",
    "\n",
    "Final score: 0.245683"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42152157",
   "metadata": {},
   "source": [
    "**Observation:** the performance of the \"text2text-generation\" task type is not as good as the \"translation\" task type used above, although the inference seems to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9434a-9a46-47e4-8bef-db2167fba363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a34017c-65d9-4d25-81fa-c34374064a25",
   "metadata": {},
   "source": [
    "# Fine tuning with Deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f7921-1c2e-4d79-8fd6-ce7b4035deed",
   "metadata": {},
   "source": [
    "Reference: \n",
    "\n",
    "https://www.philschmid.de/fine-tune-flan-t5-deepspeed\n",
    "\n",
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/configs/ds_flan_t5_z3_config_bf16.json\n",
    "\n",
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_seq2seq_deepspeed.py\n",
    "\n",
    "Note that the Deepspeed script and configuration file used below are based on these references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ae8022-59a8-42ec-bb52-8632a9eaca53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f182a50755c74459b76c20e1ab0049ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e60911",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The command to fine tune the model with Deepspeed:\n",
    "\n",
    "```\n",
    "deepspeed --num_gpus=8 run_seq2seq_deepspeed.py \\\n",
    "    --model_id google/flan-t5-xl \\\n",
    "    --repository_id delmeng/flan-t5-xl-finetuning-translation-ds \\\n",
    "    --dataset_path training_data \\\n",
    "    --epochs 3 \\\n",
    "    --per_device_train_batch_size 96 \\\n",
    "    --per_device_eval_batch_size 96 \\\n",
    "    --generation_max_length 273 \\\n",
    "    --lr 1e-4 \\\n",
    "    --deepspeed deepspeed_config.json\n",
    "```\n",
    "\n",
    "(Note that it took around 3.5h for this training job.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306cd73-a895-4087-8851-6e4fa7e0b659",
   "metadata": {},
   "source": [
    "## Use the fine-tuned model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f2672",
   "metadata": {},
   "source": [
    "After the training, the model was uploaded to my Hugging Face repository, so I can download and use it.\n",
    "\n",
    "https://huggingface.co/delmeng/flan-t5-xl-finetuning-translation-ds/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e42ab4a4-744a-4cc0-97dc-38e31a30849c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42307e4014a24f48a329c0087520c403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repository_id = \"delmeng/flan-t5-xl-finetuning-translation-ds\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(repository_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(repository_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec118b2c-e894-4367-bd1b-7b1b927d1498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48735560-4704-4ec4-99ec-6914621b24ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304f1ef576234d69a585513dd317c4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/envs/ml/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1013: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe_ft = pipeline(\"translation\", model = repository_id, max_length=tokenizer.model_max_length, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1d09a",
   "metadata": {},
   "source": [
    "### Evaluation on a small subset of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "332c57b3-cd3a-4f8a-aca7-7a5821d2ccd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'input_to_translate', 'label', 'gender', 'language_pair', 'prompt'],\n",
       "    num_rows: 96\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 96\n",
    "train_ds_test = datasets.Dataset.from_pandas(training_features.head(sample_size), split=\"train\")\n",
    "train_ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c6f66-624f-4475-80ec-9a6a5c61eed4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "prediction = pd.DataFrame({\"ID\": pd.Series(dtype=\"int\"),\n",
    "                   \"predicted_label\": pd.Series(dtype=\"str\")})\n",
    "batch_size = 8\n",
    "\n",
    "for out in tqdm.tqdm(pipe_ft(KeyDataset(train_ds_test, \"prompt\"), batch_size=batch_size),total=len(train_ds_test)):\n",
    "\n",
    "    generated_text = out[0]['translation_text']\n",
    "    predicted_labels.append(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08f71d-b65f-4f51-b650-627f3df407ec",
   "metadata": {},
   "source": [
    "Tried different batch size here => batch = 8 is a good choice.\n",
    "\n",
    "batch = 48: This step is super slow!! Give up!!  \n",
    "batch = 16: This step is super slow!! Give up!!  \n",
    "batch = 8: 3min 48s  100%|██████████| 96/96 [03:48<00:00,  2.38s/it]  \n",
    "batch = 1: 4min 100%|██████████| 96/96 [04:03<00:00,  2.53s/it]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "708b6b58-3d52-45db-92df-1a5528dab6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction[\"ID\"] = training_features.iloc[0:sample_size][\"ID\"]\n",
    "prediction[\"predicted_label\"] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c737601a-c5cc-4e9c-a412-6079057c1bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.37039246550402644\n",
      "Different genders: [0.37006333542410835, 0.37087350177467604]\n",
      "Final score: 0.3699873823287426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3699873823287426"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_custom(\n",
    "    training_features.iloc[0:sample_size][\"label\"], \n",
    "    prediction[\"predicted_label\"], \n",
    "    training_features.iloc[0:sample_size][\"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b266f7",
   "metadata": {},
   "source": [
    "### Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd7267-af01-47f9-a191-71196e5a11b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 177/3000 [08:04<1:41:23,  2.15s/it]"
     ]
    }
   ],
   "source": [
    "# note: this didn't work, it stuck at 6%-ish and couldn't finish\n",
    "\n",
    "predicted_labels = []\n",
    "test_prediction = pd.DataFrame({\"ID\": pd.Series(dtype=\"int\"), \"label\": pd.Series(dtype=\"str\")})\n",
    "batch_size = 8\n",
    "\n",
    "for out in tqdm.tqdm(pipe_ft(KeyDataset(test_ds_raw, \"prompt\"), batch_size=batch_size),total=len(test_ds_raw)):\n",
    "    generated_text = out[0]['translation_text']\n",
    "    predicted_labels.append(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd54f5d-62f8-4f06-ab96-185aedc7d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3000 [00:44<3:46:10,  4.54s/it]/opt/conda/envs/ml/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 3000/3000 [2:30:17<00:00,  3.01s/it]  \n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "test_prediction = pd.DataFrame({\"ID\": pd.Series(dtype=\"int\"), \"label\": pd.Series(dtype=\"str\")})\n",
    "\n",
    "for input_text in tqdm.tqdm(KeyDataset(test_ds_raw, \"prompt\")):\n",
    "    generated_text = pipe_ft(input_text)[0]['translation_text']\n",
    "    predicted_labels.append(generated_text)\n",
    "\n",
    "test_prediction[\"ID\"] = test_features[\"ID\"]\n",
    "test_prediction[\"label\"] = predicted_labels\n",
    "test_prediction.to_csv(\"t5_xl_finetuned_translation_ds-10142023.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55825835-b7eb-4174-a4e3-3acf36555693",
   "metadata": {},
   "source": [
    "\n",
    "The inference took 2.5 hours.\n",
    "\n",
    "Final score: 0.262809"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729a2c4",
   "metadata": {},
   "source": [
    "**Observation:** fine-tuning with Deepspeed didn't help with the fine-tuning in my case. In fact, somehow it slowed down the process. This could be caused by some configuration issue? The overall translation performance is similar to without Deepspeed though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e574f-f1ca-4db5-bf3c-d9461eb38ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
